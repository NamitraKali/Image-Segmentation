cuda
2975 2975
(4, 256, 256, 1)
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
0 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
1 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
2 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
3 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
4 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Epoch: 0:   0%|          | 0/595.0 [00:00<?, ?it/s]/home/namitrak/anaconda3/envs/rl_gan/lib/python3.8/site-packages/apex/amp/wrap.py:101: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  return orig_fn(arg0, *args, **kwargs)
Epoch: 0:   0%|          | 1/595.0 [00:02<25:02,  2.53s/it]Epoch: 0:   0%|          | 2/595.0 [00:02<18:39,  1.89s/it]Epoch: 0:   1%|          | 3/595.0 [00:03<14:04,  1.43s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch: 0:   1%|          | 4/595.0 [00:03<10:46,  1.09s/it]Epoch: 0:   1%|          | 5/595.0 [00:03<08:34,  1.15it/s]Epoch: 0:   1%|          | 6/595.0 [00:04<07:01,  1.40it/s]Epoch: 0:   1%|          | 7/595.0 [00:04<05:58,  1.64it/s]Epoch: 0:   1%|▏         | 8/595.0 [00:05<05:11,  1.88it/s]Epoch: 0:   2%|▏         | 9/595.0 [00:05<04:39,  2.10it/s]Epoch: 0:   2%|▏         | 10/595.0 [00:05<04:16,  2.28it/s]Epoch: 0:   2%|▏         | 11/595.0 [00:06<04:00,  2.43it/s]Epoch: 0:   2%|▏         | 12/595.0 [00:06<03:48,  2.55it/s]Epoch: 0:   2%|▏         | 13/595.0 [00:06<03:40,  2.63it/s]Epoch: 0:   2%|▏         | 14/595.0 [00:07<03:36,  2.68it/s]Epoch: 0:   3%|▎         | 15/595.0 [00:07<03:32,  2.73it/s]Epoch: 0:   3%|▎         | 16/595.0 [00:07<03:34,  2.70it/s]Epoch: 0:   3%|▎         | 17/595.0 [00:08<03:37,  2.66it/s]Epoch: 0:   3%|▎         | 18/595.0 [00:08<03:40,  2.62it/s]Epoch: 0:   3%|▎         | 19/595.0 [00:09<03:42,  2.59it/s]Epoch: 0:   3%|▎         | 20/595.0 [00:09<03:41,  2.59it/s]Epoch: 0:   4%|▎         | 21/595.0 [00:09<03:42,  2.58it/s]Epoch: 0:   4%|▎         | 22/595.0 [00:10<03:43,  2.57it/s]Epoch: 0:   4%|▍         | 23/595.0 [00:10<03:39,  2.61it/s]Epoch: 0:   4%|▍         | 24/595.0 [00:10<03:40,  2.59it/s]Epoch: 0:   4%|▍         | 25/595.0 [00:11<03:40,  2.59it/s]Epoch: 0:   4%|▍         | 26/595.0 [00:11<03:40,  2.58it/s]Epoch: 0:   5%|▍         | 27/595.0 [00:12<03:40,  2.58it/s]Epoch: 0:   5%|▍         | 28/595.0 [00:12<03:40,  2.57it/s]Epoch: 0:   5%|▍         | 29/595.0 [00:12<03:40,  2.57it/s]Epoch: 0:   5%|▌         | 30/595.0 [00:13<03:39,  2.58it/s]Epoch: 0:   5%|▌         | 31/595.0 [00:13<03:40,  2.56it/s]Epoch: 0:   5%|▌         | 32/595.0 [00:14<03:36,  2.60it/s]Epoch: 0:   6%|▌         | 33/595.0 [00:14<03:37,  2.59it/s]Epoch: 0:   6%|▌         | 34/595.0 [00:14<03:30,  2.66it/s]Epoch: 0:   6%|▌         | 35/595.0 [00:15<03:25,  2.72it/s]Epoch: 0:   6%|▌         | 36/595.0 [00:15<03:21,  2.77it/s]Epoch: 0:   6%|▌         | 37/595.0 [00:15<03:19,  2.80it/s]Epoch: 0:   6%|▋         | 38/595.0 [00:16<03:17,  2.81it/s]Epoch: 0:   7%|▋         | 39/595.0 [00:16<03:16,  2.83it/s]Epoch: 0:   7%|▋         | 40/595.0 [00:16<03:15,  2.84it/s]Epoch: 0:   7%|▋         | 41/595.0 [00:17<03:14,  2.85it/s]Epoch: 0:   7%|▋         | 42/595.0 [00:17<03:13,  2.85it/s]Epoch: 0:   7%|▋         | 43/595.0 [00:17<03:13,  2.85it/s]Epoch: 0:   7%|▋         | 44/595.0 [00:18<03:13,  2.85it/s]Epoch: 0:   8%|▊         | 45/595.0 [00:18<03:12,  2.85it/s]Epoch: 0:   8%|▊         | 46/595.0 [00:18<03:12,  2.86it/s]Epoch: 0:   8%|▊         | 47/595.0 [00:19<03:11,  2.86it/s]Epoch: 0:   8%|▊         | 48/595.0 [00:19<03:11,  2.86it/s]Epoch: 0:   8%|▊         | 49/595.0 [00:20<03:11,  2.85it/s]Epoch: 0:   8%|▊         | 50/595.0 [00:20<03:10,  2.86it/s]Epoch: 0:   9%|▊         | 51/595.0 [00:20<03:10,  2.86it/s]Epoch: 0:   9%|▊         | 52/595.0 [00:21<03:09,  2.86it/s]Epoch: 0:   9%|▉         | 53/595.0 [00:21<03:09,  2.86it/s]Epoch: 0:   9%|▉         | 54/595.0 [00:21<03:08,  2.86it/s]Epoch: 0:   9%|▉         | 55/595.0 [00:22<03:09,  2.86it/s]Epoch: 0:   9%|▉         | 56/595.0 [00:22<03:08,  2.86it/s]Epoch: 0:  10%|▉         | 57/595.0 [00:22<03:08,  2.86it/s]Epoch: 0:  10%|▉         | 58/595.0 [00:23<03:07,  2.87it/s]Epoch: 0:  10%|▉         | 59/595.0 [00:23<03:06,  2.87it/s]Epoch: 0:  10%|█         | 60/595.0 [00:23<03:06,  2.87it/s]Epoch: 0:  10%|█         | 61/595.0 [00:24<03:06,  2.86it/s]Epoch: 0:  10%|█         | 62/595.0 [00:24<03:06,  2.86it/s]Epoch: 0:  11%|█         | 63/595.0 [00:24<03:06,  2.86it/s]Epoch: 0:  11%|█         | 64/595.0 [00:25<03:05,  2.86it/s]Epoch: 0:  11%|█         | 65/595.0 [00:25<03:05,  2.86it/s]Epoch: 0:  11%|█         | 66/595.0 [00:25<03:05,  2.86it/s]Epoch: 0:  11%|█▏        | 67/595.0 [00:26<03:04,  2.86it/s]Epoch: 0:  11%|█▏        | 68/595.0 [00:26<03:04,  2.86it/s]Epoch: 0:  12%|█▏        | 69/595.0 [00:27<03:04,  2.85it/s]Epoch: 0:  12%|█▏        | 70/595.0 [00:27<03:03,  2.86it/s]Epoch: 0:  12%|█▏        | 71/595.0 [00:27<03:03,  2.85it/s]Epoch: 0:  12%|█▏        | 72/595.0 [00:28<03:02,  2.86it/s]Epoch: 0:  12%|█▏        | 73/595.0 [00:28<03:03,  2.85it/s]Epoch: 0:  12%|█▏        | 74/595.0 [00:28<03:02,  2.85it/s]Epoch: 0:  13%|█▎        | 75/595.0 [00:29<03:02,  2.86it/s]Epoch: 0:  13%|█▎        | 76/595.0 [00:29<03:01,  2.87it/s]Epoch: 0:  13%|█▎        | 77/595.0 [00:29<03:00,  2.86it/s]Epoch: 0:  13%|█▎        | 78/595.0 [00:30<03:00,  2.87it/s]Epoch: 0:  13%|█▎        | 79/595.0 [00:30<03:00,  2.86it/s]Epoch: 0:  13%|█▎        | 80/595.0 [00:30<03:00,  2.86it/s]Epoch: 0:  14%|█▎        | 81/595.0 [00:31<02:59,  2.86it/s]Epoch: 0:  14%|█▍        | 82/595.0 [00:31<02:59,  2.86it/s]Epoch: 0:  14%|█▍        | 83/595.0 [00:31<02:59,  2.86it/s]Epoch: 0:  14%|█▍        | 84/595.0 [00:32<02:58,  2.86it/s]Epoch: 0:  14%|█▍        | 85/595.0 [00:32<03:05,  2.76it/s]Epoch: 0:  14%|█▍        | 86/595.0 [00:33<03:02,  2.79it/s]Epoch: 0:  15%|█▍        | 87/595.0 [00:33<03:00,  2.82it/s]Epoch: 0:  15%|█▍        | 88/595.0 [00:33<02:58,  2.83it/s]Epoch: 0:  15%|█▍        | 89/595.0 [00:34<03:02,  2.78it/s]Epoch: 0:  15%|█▌        | 90/595.0 [00:34<03:06,  2.70it/s]Epoch: 0:  15%|█▌        | 91/595.0 [00:34<03:07,  2.68it/s]Epoch: 0:  15%|█▌        | 92/595.0 [00:35<03:10,  2.64it/s]Epoch: 0:  16%|█▌        | 93/595.0 [00:35<03:06,  2.69it/s]Epoch: 0:  16%|█▌        | 94/595.0 [00:35<03:02,  2.74it/s]Epoch: 0:  16%|█▌        | 95/595.0 [00:36<02:59,  2.78it/s]Epoch: 0:  16%|█▌        | 96/595.0 [00:36<02:57,  2.81it/s]Epoch: 0:  16%|█▋        | 97/595.0 [00:37<02:58,  2.80it/s]Epoch: 0:  16%|█▋        | 98/595.0 [00:37<02:56,  2.81it/s]Epoch: 0:  17%|█▋        | 99/595.0 [00:37<02:55,  2.82it/s]Epoch: 0:  17%|█▋        | 100/595.0 [00:38<02:55,  2.82it/s]Epoch: 0:  17%|█▋        | 101/595.0 [00:38<02:53,  2.84it/s]Epoch: 0:  17%|█▋        | 102/595.0 [00:38<02:52,  2.85it/s]Epoch: 0:  17%|█▋        | 103/595.0 [00:39<02:57,  2.78it/s]Epoch: 0:  17%|█▋        | 104/595.0 [00:39<03:01,  2.71it/s]Epoch: 0:  18%|█▊        | 105/595.0 [00:39<02:59,  2.73it/s]Epoch: 0:  18%|█▊        | 106/595.0 [00:40<02:56,  2.77it/s]Epoch: 0:  18%|█▊        | 107/595.0 [00:40<02:57,  2.75it/s]Epoch: 0:  18%|█▊        | 108/595.0 [00:41<03:01,  2.68it/s]Epoch: 0:  18%|█▊        | 109/595.0 [00:41<03:03,  2.64it/s]Epoch: 0:  18%|█▊        | 110/595.0 [00:41<02:59,  2.71it/s]Epoch: 0:  19%|█▊        | 111/595.0 [00:42<02:56,  2.75it/s]Epoch: 0:  19%|█▉        | 112/595.0 [00:42<02:53,  2.78it/s]Epoch: 0:  19%|█▉        | 113/595.0 [00:42<02:51,  2.80it/s]Epoch: 0:  19%|█▉        | 114/595.0 [00:43<02:50,  2.82it/s]Epoch: 0:  19%|█▉        | 115/595.0 [00:43<02:50,  2.82it/s]Epoch: 0:  19%|█▉        | 116/595.0 [00:43<02:48,  2.83it/s]Epoch: 0:  20%|█▉        | 117/595.0 [00:44<02:47,  2.85it/s]Epoch: 0:  20%|█▉        | 118/595.0 [00:44<02:47,  2.85it/s]Epoch: 0:  20%|██        | 119/595.0 [00:44<02:47,  2.85it/s]Epoch: 0:  20%|██        | 120/595.0 [00:45<02:46,  2.86it/s]Epoch: 0:  20%|██        | 121/595.0 [00:45<02:46,  2.85it/s]Epoch: 0:  21%|██        | 122/595.0 [00:45<02:45,  2.86it/s]Epoch: 0:  21%|██        | 123/595.0 [00:46<02:45,  2.86it/s]Epoch: 0:  21%|██        | 124/595.0 [00:46<02:44,  2.86it/s]Epoch: 0:  21%|██        | 125/595.0 [00:46<02:44,  2.86it/s]Epoch: 0:  21%|██        | 126/595.0 [00:47<02:43,  2.86it/s]Epoch: 0:  21%|██▏       | 127/595.0 [00:47<02:44,  2.85it/s]Epoch: 0:  22%|██▏       | 128/595.0 [00:48<02:43,  2.85it/s]Epoch: 0:  22%|██▏       | 129/595.0 [00:48<02:43,  2.85it/s]Epoch: 0:  22%|██▏       | 130/595.0 [00:48<02:42,  2.86it/s]Epoch: 0:  22%|██▏       | 131/595.0 [00:49<02:42,  2.86it/s]Epoch: 0:  22%|██▏       | 132/595.0 [00:49<02:41,  2.86it/s]Epoch: 0:  22%|██▏       | 133/595.0 [00:49<02:42,  2.85it/s]Epoch: 0:  23%|██▎       | 134/595.0 [00:50<02:41,  2.86it/s]Epoch: 0:  23%|██▎       | 135/595.0 [00:50<02:41,  2.85it/s]Epoch: 0:  23%|██▎       | 136/595.0 [00:50<02:40,  2.86it/s]Epoch: 0:  23%|██▎       | 137/595.0 [00:51<02:40,  2.85it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch: 0:  23%|██▎       | 138/595.0 [00:51<02:36,  2.93it/s]Epoch: 0:  23%|██▎       | 139/595.0 [00:51<02:38,  2.88it/s]Epoch: 0:  24%|██▎       | 140/595.0 [00:52<02:39,  2.85it/s]Epoch: 0:  24%|██▎       | 141/595.0 [00:52<02:39,  2.85it/s]Epoch: 0:  24%|██▍       | 142/595.0 [00:52<02:38,  2.86it/s]Epoch: 0:  24%|██▍       | 143/595.0 [00:53<02:38,  2.86it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch: 0:  24%|██▍       | 144/595.0 [00:53<02:37,  2.87it/s]Epoch: 0:  24%|██▍       | 145/595.0 [00:54<02:41,  2.79it/s]Epoch: 0:  25%|██▍       | 146/595.0 [00:54<02:40,  2.79it/s]Epoch: 0:  25%|██▍       | 147/595.0 [00:54<02:41,  2.77it/s]Epoch: 0:  25%|██▍       | 148/595.0 [00:55<02:45,  2.70it/s]Epoch: 0:  25%|██▌       | 149/595.0 [00:55<02:43,  2.74it/s]Epoch: 0:  25%|██▌       | 149/595.0 [00:55<02:46,  2.69it/s]
Selected optimization level O0:  Pure FP32 training.

Defaults for this optimization level are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Epoch: 0:   0%|          | 0/595.0 [00:00<?, ?it/s]Epoch: 0:   0%|          | 1/595.0 [00:00<04:17,  2.31it/s]Epoch: 0:   0%|          | 2/595.0 [00:00<04:10,  2.37it/s]Epoch: 0:   1%|          | 3/595.0 [00:01<03:58,  2.49it/s]Epoch: 0:   1%|          | 4/595.0 [00:01<03:50,  2.57it/s]Epoch: 0:   1%|          | 5/595.0 [00:01<03:44,  2.63it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4b00455820>
Traceback (most recent call last):
  File "/home/namitrak/anaconda3/envs/rl_gan/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in __del__
    self._shutdown_workers()
  File "/home/namitrak/anaconda3/envs/rl_gan/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1177, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/namitrak/anaconda3/envs/rl_gan/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/namitrak/anaconda3/envs/rl_gan/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/home/namitrak/anaconda3/envs/rl_gan/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/namitrak/anaconda3/envs/rl_gan/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt: 
Epoch: 0:   1%|          | 6/595.0 [00:25<1:12:00,  7.34s/it]Epoch: 0:   1%|          | 7/595.0 [00:25<51:26,  5.25s/it]  Epoch: 0:   1%|▏         | 8/595.0 [00:26<37:00,  3.78s/it]Epoch: 0:   1%|▏         | 8/595.0 [00:26<32:03,  3.28s/it]
