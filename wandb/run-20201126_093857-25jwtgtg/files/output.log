cuda
2975 2975
(4, 256, 256, 1)
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Epoch: 0:   0%|          | 0/595 [00:00<?, ?it/s]/home/namitrak/anaconda3/envs/rl_gan/lib/python3.8/site-packages/apex/amp/wrap.py:101: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  return orig_fn(arg0, *args, **kwargs)
Epoch: 0:   0%|          | 1/595 [00:02<25:19,  2.56s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch: 0:   0%|          | 2/595 [00:02<18:45,  1.90s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch: 0:   1%|          | 3/595 [00:03<14:08,  1.43s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch: 0:   1%|          | 4/595 [00:03<10:55,  1.11s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Epoch: 0:   1%|          | 5/595 [00:03<08:41,  1.13it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Epoch: 0:   1%|          | 6/595 [00:04<07:07,  1.38it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Epoch: 0:   1%|          | 7/595 [00:04<06:01,  1.63it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Epoch: 0:   1%|▏         | 8/595 [00:05<05:14,  1.87it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Epoch: 0:   2%|▏         | 9/595 [00:05<04:40,  2.09it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Epoch: 0:   2%|▏         | 10/595 [00:05<04:16,  2.28it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Epoch: 0:   2%|▏         | 11/595 [00:06<04:01,  2.42it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Epoch: 0:   2%|▏         | 12/595 [00:06<03:51,  2.52it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Epoch: 0:   2%|▏         | 13/595 [00:06<03:43,  2.60it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Epoch: 0:   2%|▏         | 14/595 [00:07<03:36,  2.68it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Epoch: 0:   3%|▎         | 15/595 [00:07<03:31,  2.74it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Epoch: 0:   3%|▎         | 16/595 [00:07<03:29,  2.76it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0
Epoch: 0:   3%|▎         | 17/595 [00:08<03:28,  2.77it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.5
Epoch: 0:   3%|▎         | 18/595 [00:08<03:24,  2.82it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.25
Epoch: 0:   3%|▎         | 19/595 [00:08<03:23,  2.84it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.125
Epoch: 0:   3%|▎         | 20/595 [00:09<03:21,  2.85it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0625
Epoch: 0:   4%|▎         | 21/595 [00:09<03:20,  2.86it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.03125
Epoch: 0:   4%|▎         | 22/595 [00:09<03:18,  2.89it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.015625
Epoch: 0:   4%|▍         | 23/595 [00:10<03:18,  2.88it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0078125
Epoch: 0:   4%|▍         | 24/595 [00:10<03:18,  2.88it/s]Epoch: 0:   4%|▍         | 24/595 [00:10<04:20,  2.19it/s]
