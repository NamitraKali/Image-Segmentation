cuda
2975 2975
(4, 256, 256, 1)
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Epoch: 0:   0%|          | 0/595 [00:00<?, ?it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch: 0:   0%|          | 1/595 [00:02<24:10,  2.44s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch: 0:   0%|          | 2/595 [00:02<18:02,  1.83s/it]/home/namitrak/anaconda3/envs/rl_gan/lib/python3.8/site-packages/apex/amp/wrap.py:101: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  return orig_fn(arg0, *args, **kwargs)
Epoch: 0:   1%|          | 3/595 [00:03<13:43,  1.39s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch: 0:   1%|          | 4/595 [00:03<10:34,  1.07s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Epoch: 0:   1%|          | 5/595 [00:03<08:22,  1.17it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Epoch: 0:   1%|          | 6/595 [00:04<06:54,  1.42it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Epoch: 0:   1%|          | 7/595 [00:04<05:50,  1.68it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Epoch: 0:   1%|▏         | 8/595 [00:04<05:07,  1.91it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Epoch: 0:   2%|▏         | 9/595 [00:05<04:41,  2.08it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Epoch: 0:   2%|▏         | 10/595 [00:05<04:14,  2.30it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Epoch: 0:   2%|▏         | 11/595 [00:05<03:56,  2.47it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Epoch: 0:   2%|▏         | 12/595 [00:06<03:42,  2.62it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Epoch: 0:   2%|▏         | 13/595 [00:06<03:33,  2.73it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Epoch: 0:   2%|▏         | 14/595 [00:06<03:26,  2.81it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Epoch: 0:   3%|▎         | 15/595 [00:07<03:21,  2.88it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Epoch: 0:   3%|▎         | 16/595 [00:07<03:18,  2.92it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0
Epoch: 0:   3%|▎         | 17/595 [00:07<03:18,  2.91it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.5
Epoch: 0:   3%|▎         | 18/595 [00:08<03:21,  2.87it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.25
Epoch: 0:   3%|▎         | 19/595 [00:08<03:21,  2.86it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.125
Epoch: 0:   3%|▎         | 20/595 [00:09<03:22,  2.84it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0625
Epoch: 0:   4%|▎         | 21/595 [00:09<03:23,  2.82it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.03125
Epoch: 0:   4%|▎         | 22/595 [00:09<03:22,  2.83it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.015625
Epoch: 0:   4%|▍         | 23/595 [00:10<03:23,  2.82it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0078125
Epoch: 0:   4%|▍         | 24/595 [00:10<03:21,  2.84it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00390625
Epoch: 0:   4%|▍         | 25/595 [00:10<03:18,  2.87it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.001953125
Epoch: 0:   4%|▍         | 26/595 [00:11<03:16,  2.90it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0009765625
Epoch: 0:   5%|▍         | 27/595 [00:11<03:14,  2.92it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00048828125
Epoch: 0:   5%|▍         | 28/595 [00:11<03:12,  2.94it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.000244140625
Epoch: 0:   5%|▍         | 29/595 [00:12<03:11,  2.96it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0001220703125
Epoch: 0:   5%|▌         | 30/595 [00:12<03:10,  2.97it/s]Epoch: 0:   5%|▌         | 30/595 [00:12<04:01,  2.34it/s]
