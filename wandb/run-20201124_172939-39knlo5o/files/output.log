cuda
2975 2975
(4, 256, 256, 1)
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
0 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
1 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
2 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
3 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
4 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
Selected optimization level O0:  Pure FP32 training.

Defaults for this optimization level are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Epoch: 0:   0%|          | 0/595 [00:00<?, ?it/s]optims/ralamb.py:47: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
Epoch: 0:   0%|          | 1/595 [00:01<19:39,  1.99s/it]Epoch: 0:   0%|          | 2/595 [00:02<14:50,  1.50s/it]Epoch: 0:   1%|          | 3/595 [00:02<11:19,  1.15s/it]Epoch: 0:   1%|          | 4/595 [00:03<08:51,  1.11it/s]Epoch: 0:   1%|          | 5/595 [00:03<07:08,  1.38it/s]Epoch: 0:   1%|          | 6/595 [00:03<05:57,  1.65it/s]Epoch: 0:   1%|          | 7/595 [00:03<05:08,  1.90it/s]Epoch: 0:   1%|▏         | 8/595 [00:04<04:39,  2.10it/s]Epoch: 0:   2%|▏         | 9/595 [00:04<04:13,  2.31it/s]Epoch: 0:   2%|▏         | 10/595 [00:05<03:58,  2.45it/s]Epoch: 0:   2%|▏         | 11/595 [00:05<03:50,  2.53it/s]Epoch: 0:   2%|▏         | 12/595 [00:05<03:44,  2.59it/s]Epoch: 0:   2%|▏         | 13/595 [00:06<03:40,  2.64it/s]Epoch: 0:   2%|▏         | 14/595 [00:06<03:34,  2.71it/s]Epoch: 0:   3%|▎         | 15/595 [00:06<03:32,  2.73it/s]Epoch: 0:   3%|▎         | 16/595 [00:07<03:24,  2.83it/s]Epoch: 0:   3%|▎         | 17/595 [00:07<03:18,  2.90it/s]Epoch: 0:   3%|▎         | 18/595 [00:07<03:15,  2.96it/s]Epoch: 0:   3%|▎         | 19/595 [00:08<03:12,  3.00it/s]Epoch: 0:   3%|▎         | 20/595 [00:08<03:09,  3.03it/s]Epoch: 0:   4%|▎         | 21/595 [00:08<03:07,  3.05it/s]Epoch: 0:   4%|▎         | 22/595 [00:09<03:10,  3.00it/s]Epoch: 0:   4%|▍         | 23/595 [00:09<03:15,  2.92it/s]Epoch: 0:   4%|▍         | 24/595 [00:09<03:12,  2.96it/s]Epoch: 0:   4%|▍         | 25/595 [00:10<03:09,  3.00it/s]Epoch: 0:   4%|▍         | 26/595 [00:10<03:07,  3.04it/s]Epoch: 0:   5%|▍         | 27/595 [00:10<03:05,  3.06it/s]Epoch: 0:   5%|▍         | 28/595 [00:11<03:03,  3.08it/s]Epoch: 0:   5%|▍         | 29/595 [00:11<03:03,  3.09it/s]Epoch: 0:   5%|▌         | 30/595 [00:11<03:02,  3.10it/s]Epoch: 0:   5%|▌         | 31/595 [00:12<03:01,  3.10it/s]Epoch: 0:   5%|▌         | 32/595 [00:12<03:00,  3.11it/s]Epoch: 0:   6%|▌         | 33/595 [00:12<03:00,  3.11it/s]Epoch: 0:   6%|▌         | 34/595 [00:13<03:00,  3.11it/s]Epoch: 0:   6%|▌         | 34/595 [00:13<03:36,  2.59it/s]
Selected optimization level O0:  Pure FP32 training.

Defaults for this optimization level are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O0
cast_model_type        : torch.float32
patch_torch_functions  : False
keep_batchnorm_fp32    : None
master_weights         : False
loss_scale             : 1.0
Epoch: 0:   0%|          | 0/595 [00:00<?, ?it/s]Epoch: 0:   0%|          | 0/595 [00:00<?, ?it/s]
