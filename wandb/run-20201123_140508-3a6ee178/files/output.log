cuda
2975 2975
(4, 256, 256, 1)
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
torch.Size([3, 256, 256]) torch.Size([1, 256, 256])
0 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
1 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
2 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
3 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
4 torch.Size([5, 3, 256, 256]) torch.Size([5, 1, 256, 256])
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Epoch: 0:   0%|          | 0/595.0 [00:00<?, ?it/s]/home/namitrak/anaconda3/envs/rl_gan/lib/python3.8/site-packages/apex/amp/wrap.py:101: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  return orig_fn(arg0, *args, **kwargs)
Epoch: 0:   0%|          | 1/595.0 [00:02<26:27,  2.67s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Epoch: 0:   0%|          | 2/595.0 [00:03<19:36,  1.98s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Epoch: 0:   1%|          | 3/595.0 [00:03<14:46,  1.50s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Epoch: 0:   1%|          | 4/595.0 [00:03<11:19,  1.15s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Epoch: 0:   1%|          | 5/595.0 [00:04<08:56,  1.10it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Epoch: 0:   1%|          | 6/595.0 [00:04<07:16,  1.35it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Epoch: 0:   1%|          | 7/595.0 [00:04<06:06,  1.61it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Epoch: 0:   1%|▏         | 8/595.0 [00:05<05:16,  1.85it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Epoch: 0:   2%|▏         | 9/595.0 [00:05<04:39,  2.10it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Epoch: 0:   2%|▏         | 10/595.0 [00:05<04:11,  2.32it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Epoch: 0:   2%|▏         | 11/595.0 [00:06<03:54,  2.50it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Epoch: 0:   2%|▏         | 12/595.0 [00:06<03:44,  2.60it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Epoch: 0:   2%|▏         | 13/595.0 [00:06<03:35,  2.70it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Epoch: 0:   2%|▏         | 14/595.0 [00:07<03:27,  2.80it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Epoch: 0:   3%|▎         | 15/595.0 [00:07<03:21,  2.88it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.0
Epoch: 0:   3%|▎         | 16/595.0 [00:07<03:20,  2.89it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.0
Epoch: 0:   3%|▎         | 17/595.0 [00:08<03:20,  2.89it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.5
Epoch: 0:   3%|▎         | 18/595.0 [00:08<03:18,  2.91it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.25
Epoch: 0:   3%|▎         | 19/595.0 [00:08<03:15,  2.95it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.125
Epoch: 0:   3%|▎         | 20/595.0 [00:09<03:12,  2.99it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0625
Epoch: 0:   4%|▎         | 21/595.0 [00:09<03:10,  3.01it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.03125
Epoch: 0:   4%|▎         | 22/595.0 [00:09<03:08,  3.03it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.015625
Epoch: 0:   4%|▍         | 23/595.0 [00:10<03:11,  2.99it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0078125
Epoch: 0:   4%|▍         | 24/595.0 [00:10<03:12,  2.96it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00390625
Epoch: 0:   4%|▍         | 25/595.0 [00:10<03:11,  2.98it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.001953125
Epoch: 0:   4%|▍         | 26/595.0 [00:11<03:09,  3.00it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0009765625
Epoch: 0:   5%|▍         | 27/595.0 [00:11<03:07,  3.02it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.00048828125
Epoch: 0:   5%|▍         | 28/595.0 [00:11<03:06,  3.04it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.000244140625
Epoch: 0:   5%|▍         | 29/595.0 [00:12<03:08,  3.01it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 0.0001220703125
Epoch: 0:   5%|▌         | 30/595.0 [00:12<03:10,  2.97it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 6.103515625e-05
Epoch: 0:   5%|▌         | 31/595.0 [00:12<03:10,  2.96it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.0517578125e-05
Epoch: 0:   5%|▌         | 32/595.0 [00:13<03:08,  2.99it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.52587890625e-05
Epoch: 0:   6%|▌         | 33/595.0 [00:13<03:06,  3.01it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.62939453125e-06
Epoch: 0:   6%|▌         | 34/595.0 [00:13<03:04,  3.04it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.814697265625e-06
Epoch: 0:   6%|▌         | 35/595.0 [00:14<03:05,  3.02it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.9073486328125e-06
Epoch: 0:   6%|▌         | 36/595.0 [00:14<03:08,  2.97it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.5367431640625e-07
Epoch: 0:   6%|▌         | 37/595.0 [00:14<03:08,  2.97it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.76837158203125e-07
Epoch: 0:   6%|▋         | 38/595.0 [00:15<03:05,  3.00it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.384185791015625e-07
Epoch: 0:   7%|▋         | 39/595.0 [00:15<03:03,  3.02it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1920928955078125e-07
Epoch: 0:   7%|▋         | 40/595.0 [00:15<03:05,  2.99it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.960464477539063e-08
Epoch: 0:   7%|▋         | 41/595.0 [00:16<03:06,  2.97it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9802322387695312e-08
Epoch: 0:   7%|▋         | 42/595.0 [00:16<03:04,  3.00it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4901161193847656e-08
Epoch: 0:   7%|▋         | 43/595.0 [00:16<03:03,  3.02it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.450580596923828e-09
Epoch: 0:   7%|▋         | 44/595.0 [00:17<03:01,  3.04it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.725290298461914e-09
Epoch: 0:   8%|▊         | 45/595.0 [00:17<03:00,  3.05it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.862645149230957e-09
Epoch: 0:   8%|▊         | 46/595.0 [00:17<02:59,  3.06it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.313225746154785e-10
Epoch: 0:   8%|▊         | 47/595.0 [00:18<03:02,  3.00it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.656612873077393e-10
Epoch: 0:   8%|▊         | 48/595.0 [00:18<03:04,  2.96it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.3283064365386963e-10
Epoch: 0:   8%|▊         | 49/595.0 [00:18<03:06,  2.93it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1641532182693481e-10
Epoch: 0:   8%|▊         | 50/595.0 [00:19<03:06,  2.92it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.820766091346741e-11
Epoch: 0:   9%|▊         | 51/595.0 [00:19<03:07,  2.90it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.9103830456733704e-11
Epoch: 0:   9%|▊         | 52/595.0 [00:19<03:07,  2.89it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.4551915228366852e-11
Epoch: 0:   9%|▉         | 53/595.0 [00:20<03:05,  2.93it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 7.275957614183426e-12
Epoch: 0:   9%|▉         | 54/595.0 [00:20<03:01,  2.98it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 3.637978807091713e-12
Epoch: 0:   9%|▉         | 55/595.0 [00:20<03:00,  2.99it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.8189894035458565e-12
Epoch: 0:   9%|▉         | 56/595.0 [00:21<02:58,  3.02it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 9.094947017729282e-13
Epoch: 0:  10%|▉         | 57/595.0 [00:21<02:56,  3.04it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.547473508864641e-13
Epoch: 0:  10%|▉         | 58/595.0 [00:21<02:55,  3.06it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.2737367544323206e-13
Epoch: 0:  10%|▉         | 59/595.0 [00:22<02:54,  3.07it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1.1368683772161603e-13
Epoch: 0:  10%|█         | 60/595.0 [00:22<02:53,  3.09it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 5.684341886080802e-14
Epoch: 0:  10%|█         | 61/595.0 [00:22<02:53,  3.09it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2.842170943040401e-14
Epoch: 0:  10%|█         | 62/595.0 [00:23<02:52,  3.09it/s]Epoch: 0:  10%|█         | 62/595.0 [00:23<03:19,  2.67it/s]
